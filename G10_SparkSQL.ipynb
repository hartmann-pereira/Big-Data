{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Spark SQL Notebook\n",
    "\n",
    "The final output gives for each parameter the best and worse county and its values, also gives all the counties if the value is the same, because I assumed a analysis on this dataset would be best if shown all the counties with worse and best value, and not just one for each instance.\n",
    "This notebook exemplifies the execution of a Spark program in Python, using the SQL interface.\n",
    "In this example, spark runs in standalone mode and reads data from the local filesystem, while in cluster mode data is read typically from HDFS dsitributed file system.\n",
    "\n",
    "Spark documentation available at:\n",
    "https://spark.apache.org/docs/2.3.1/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please first run this Download the dataset \n",
    "\n",
    "Dataset is being downloaded from a dropbox link\n",
    "Contains a small subset of the original dataset\n",
    "Dataset contains the information from air quality monitoring facilities across the U.S.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-23 18:33:53--  https://www.dropbox.com/s/4jxfdsgn2tdo7zo/epa_hap_daily_summary-small.csv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.68.18, 2620:100:6024:18::a27d:4412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.68.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/4jxfdsgn2tdo7zo/epa_hap_daily_summary-small.csv [following]\n",
      "--2021-12-23 18:33:56--  https://www.dropbox.com/s/raw/4jxfdsgn2tdo7zo/epa_hap_daily_summary-small.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uca67c4e5b8f5220e50ad5ea3522.dl.dropboxusercontent.com/cd/0/inline/BcZSQ_YIYVSI5ZlTzREl7SnyFkOckGZLefhtPtp0rlG_NAiJDUFp8CpoU9y2BATw8SKbsUtNjDKZC03bgTnCy5E08CPAaaZCCP_n6W1bdyONegadn642xGhO_jL2TpymkT4V3QgW5XI6RZDDj7cgBHIP/file# [following]\n",
      "--2021-12-23 18:33:56--  https://uca67c4e5b8f5220e50ad5ea3522.dl.dropboxusercontent.com/cd/0/inline/BcZSQ_YIYVSI5ZlTzREl7SnyFkOckGZLefhtPtp0rlG_NAiJDUFp8CpoU9y2BATw8SKbsUtNjDKZC03bgTnCy5E08CPAaaZCCP_n6W1bdyONegadn642xGhO_jL2TpymkT4V3QgW5XI6RZDDj7cgBHIP/file\n",
      "Resolving uca67c4e5b8f5220e50ad5ea3522.dl.dropboxusercontent.com (uca67c4e5b8f5220e50ad5ea3522.dl.dropboxusercontent.com)... 162.125.68.15, 2620:100:6024:15::a27d:440f\n",
      "Connecting to uca67c4e5b8f5220e50ad5ea3522.dl.dropboxusercontent.com (uca67c4e5b8f5220e50ad5ea3522.dl.dropboxusercontent.com)|162.125.68.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 123020165 (117M) [text/plain]\n",
      "Saving to: ‘epa_hap_daily_summary_small’\n",
      "\n",
      "epa_hap_daily_summa 100%[===================>] 117.32M  5.40MB/s    in 1m 42s  \n",
      "\n",
      "2021-12-23 18:35:39 (1.15 MB/s) - ‘epa_hap_daily_summary_small’ saved [123020165/123020165]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O epa_hap_daily_summary_small https://www.dropbox.com/s/4jxfdsgn2tdo7zo/epa_hap_daily_summary-small.csv?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before doing anything in this notebook in order to answer some questions, we need to import specific libraries and initiate a spark session locally..\n",
    "### Some initial exploratory analysis is required in order to better understand what data we are going to manipulate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('Q2').getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('epa_hap_daily_summary_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.option(\"header\",\"true\").csv(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, state_code: string, county_code: string, site_num: string, parameter_code: string, poc: string, latitude: string, longitude: string, datum: string, parameter_name: string, sample_duration: string, pollutant_standard: string, date_local: string, units_of_measure: string, event_type: string, observation_count: string, observation_percent: string, arithmetic_mean: string, first_max_value: string, first_max_hour: string, aqi: string, method_code: string, method_name: string, local_site_name: string, address: string, state_name: string, county_name: string, city_name: string, cbsa_name: string, date_of_last_change: string]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering Question 1 using SparkSQL, Which states have more/less monitors? (Rank states!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_monitors = df = spark.read.csv(lines, inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Registers this DataFrame as a temporary table using the given name.\n",
    "\n",
    "###### The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_monitors.registerTempTable(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### After creating my temporary table I want to query it, to show only the distinct occurences of state and address, and \n",
    "###### and after that I want to group by state name column and count the number of occurences, sorting it in descending order, showing all 54 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          state_name|count|\n",
      "+--------------------+-----+\n",
      "|          California|  161|\n",
      "|               Texas|  132|\n",
      "|           Minnesota|   94|\n",
      "|                Ohio|   89|\n",
      "|            Michigan|   83|\n",
      "|            New York|   66|\n",
      "|      South Carolina|   64|\n",
      "|             Montana|   60|\n",
      "|        Pennsylvania|   60|\n",
      "|             Florida|   57|\n",
      "|             Indiana|   52|\n",
      "|            Colorado|   50|\n",
      "|      North Carolina|   49|\n",
      "|            Illinois|   49|\n",
      "|          Washington|   42|\n",
      "|           Louisiana|   40|\n",
      "|             Arizona|   38|\n",
      "|              Kansas|   37|\n",
      "|             Georgia|   34|\n",
      "|              Oregon|   31|\n",
      "|            Kentucky|   30|\n",
      "|             Alabama|   29|\n",
      "|           Tennessee|   28|\n",
      "|           Wisconsin|   24|\n",
      "|          New Jersey|   23|\n",
      "|               Maine|   21|\n",
      "|             Vermont|   21|\n",
      "|         Mississippi|   21|\n",
      "|            Virginia|   20|\n",
      "|            Oklahoma|   20|\n",
      "|       Massachusetts|   19|\n",
      "|                Iowa|   18|\n",
      "|               Idaho|   17|\n",
      "|            Maryland|   17|\n",
      "|          New Mexico|   17|\n",
      "|   Country Of Mexico|   16|\n",
      "|         Connecticut|   15|\n",
      "|            Missouri|   15|\n",
      "|                Utah|   12|\n",
      "|              Alaska|   12|\n",
      "|        Rhode Island|   12|\n",
      "|       New Hampshire|   12|\n",
      "|       West Virginia|   10|\n",
      "|            Arkansas|   10|\n",
      "|             Wyoming|    9|\n",
      "|              Nevada|    8|\n",
      "|        North Dakota|    7|\n",
      "|        South Dakota|    7|\n",
      "|         Puerto Rico|    6|\n",
      "|            Delaware|    6|\n",
      "|            Nebraska|    6|\n",
      "|District Of Columbia|    5|\n",
      "|              Hawaii|    5|\n",
      "|      Virgin Islands|    4|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_address=spark.sql(\"select state_name, address from data\").distinct().groupBy(\"state_name\").count().sort(\"count\", ascending=False).show(54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering Question 2 using SparkSQL, Which counties have the best/worst air quality? (Rank counties considering pollutants’ level!)\n",
    "#### In this question my approach was to do a window view and partition by the parameter code because I figured the most value would be taken from figuring out what are the worst and best counties considering the different pollutants, this way it would be possible to quickly figure out for a specific dangerous pollutant what are the counties that need the most interventnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_param_value = df = spark.read.csv(lines,inferSchema =True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_param_value_small = county_param_value.drop('state_code','county_code','site_num','poc','latitude','longitude','datum','parameter_name','sample_duration','pollutant_standard','date_local','units_of_measure','event_type','observation_count','observation_percent','first_max_value','first_max_hour','aqi','method_code','method_name','local_site_name','address','state_name','city_name','cbsa_name','date_of_last_change')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = W.partitionBy(\"parameter_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to present the best and worst county for each pollutant, also showing the arithmetic mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+---------------+\n",
      "|parameter_code|    county_name|arithmetic_mean|\n",
      "+--------------+---------------+---------------+\n",
      "|         12103|          Roane|         1.0E-5|\n",
      "|         12103|Lewis and Clark|            1.2|\n",
      "|         43817|          Kings|          20.04|\n",
      "|         43817|      San Diego|          0.004|\n",
      "|         43817|          Marin|          0.004|\n",
      "|         43218|         Denton|       0.004545|\n",
      "|         43218|           Wise|       0.004545|\n",
      "|         43218|       Imperial|          128.0|\n",
      "|         43829|           Lake|          15.28|\n",
      "|         43829|        Suffolk|          0.003|\n",
      "|         82112|           Mesa|          107.4|\n",
      "|         82112|        Fayette|           0.01|\n",
      "|         82112|         Carter|           0.01|\n",
      "|         43831|        Passaic|           0.01|\n",
      "|         43831|          Anoka|           0.01|\n",
      "+--------------+---------------+---------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "county_param_value_small.filter(county_param_value_small.arithmetic_mean != 0).withColumn(\"max_arithmetic_mean\", F.max(\"arithmetic_mean\").over(window_spec))\\\n",
    "  .withColumn(\"min_arithmetic_mean\", F.min(\"arithmetic_mean\").over(window_spec))\\\n",
    "  .filter((F.col(\"arithmetic_mean\") == F.col(\"max_arithmetic_mean\")) | (F.col(\"arithmetic_mean\") == F.col(\"min_arithmetic_mean\")))\\\n",
    "  .select(\"parameter_code\", \"county_name\", \"arithmetic_mean\").distinct().show(15)\n",
    "  #filter de zeros? maybe não quero isto porque não quero discartar o que têm valor 0 da minha análise...\n",
    "  #df = df.filter(df.A != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering Question 3 using SparkSQL, Which states have the best/worst air quality? (Rank counties considering pollutants’ level!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_param_value_small = df.drop('state_code','county_code','site_num','poc','latitude','longitude','datum','parameter_name','sample_duration','pollutant_standard','date_local','units_of_measure','event_type','observation_count','observation_percent','first_max_value','first_max_hour','aqi','method_code','method_name','local_site_name','address','county_name','city_name','cbsa_name','date_of_last_change')\n",
    "# i just want these columns: 'parameter_code','state_name','arithmetic_mean' im not gonna use select \n",
    "#because i think it gives me an object noneType not a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = W.partitionBy(\"parameter_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+---------------+\n",
      "|parameter_code|  state_name|arithmetic_mean|\n",
      "+--------------+------------+---------------+\n",
      "|         43831|  California|            3.3|\n",
      "|         43831|  California|           1.23|\n",
      "|         43509|     Indiana|              1|\n",
      "|         43509|     Indiana|            7.5|\n",
      "|         43509|   Wisconsin|              1|\n",
      "|         82128|      Kansas|          2.647|\n",
      "|         82128|     Montana|           1.22|\n",
      "|         82132|    Michigan|              1|\n",
      "|         82132|     Florida|              1|\n",
      "|         82132|  California|              1|\n",
      "|         82132|      Kansas|              1|\n",
      "|         82132|    New York|              1|\n",
      "|         82132|Rhode Island|              1|\n",
      "|         82132|   Minnesota|              1|\n",
      "|         82132|    Kentucky|              1|\n",
      "+--------------+------------+---------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_param_value_small.filter(state_param_value_small.arithmetic_mean != 0).withColumn(\"max_arithmetic_mean\", F.max(\"arithmetic_mean\").over(window_spec))\\\n",
    "  .withColumn(\"min_arithmetic_mean\", F.min(\"arithmetic_mean\").over(window_spec))\\\n",
    "  .filter((F.col(\"arithmetic_mean\") == F.col(\"max_arithmetic_mean\")) | (F.col(\"arithmetic_mean\") == F.col(\"min_arithmetic_mean\")))\\\n",
    "  .select(\"parameter_code\", \"state_name\", \"arithmetic_mean\").distinct().show(15)\n",
    "  #filter de zeros? maybe não quero isto porque não quero discartar o que têm valor 0 da minha análise...\n",
    "  #df = df.filter(df.A != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
